{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of this week's riots in Capitol Hill and the subsequent ban of Donald Trump's account on Twitter I ventured to do an exploratory analysis of the public tweets about Donald Trump from January 5 to January 8 2021.\n",
    "\n",
    "In the first part of this notebook I am scraping Twitter posts that include the keyword 'Trump' for each of the days that I am interested in and saving the result set of the API search in CSV file and then reading that into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import pandas as pd\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below I am connecting to my personal Twitter developer key and thus I will hide the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
from IPython.display import HTML
from IPython.display import display

# Taken from https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook
tag = HTML('''<script>
code_show=true; 
function code_toggle() {
    if (code_show){
        $('div.cell.code_cell.rendered.selected div.input').hide();
    } else {
        $('div.cell.code_cell.rendered.selected div.input').show();
    }
    code_show = !code_show
} 

$( document ).ready(code_toggle);
</script>

To show/hide this cell's raw code input, click <a href="javascript:code_toggle()">here</a>.''')
display(tag)

############### Write code below ##################
   "outputs": [],
   "source": [
    "consumer_key = \"lMz5xkoWylvXYgaXEHDbTYOZY\"\n",
    "consumer_secret = \"hhyZrEcenqk1u0hYdfJRZooXerRvw03ENxSUCSxaIM2KS1MC6x\"\n",
    "access_token = \"1117867633531994112-yDgvXK3fKsxGjuve9gT69F4bGJCBTf\"\n",
    "access_token_secret = \"yxqf7O71y1TwXMUoEQrCZBhmWoqDtK1gINJj4qZz1pVTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Extracting tweets that include keyword 'Trump' from Jan 5 2021\n",
    "tweets=[] #to hold the extracted tweets\n",
    "search = \"#Trump -filter:retweets\"\n",
    "for tweet in tweepy.Cursor(api.search, q=search, limit=None, tweet_mode='extended', lang=\"en\", since=\"2021-01-05\", until=\"2021-01-06\").items(50000):\n",
    "    \n",
    "    try:\n",
    "        data=[tweet.user._json['screen_name'], tweet.full_text]\n",
    "        tweets.append(data)\n",
    "        \n",
    "    except tweepy.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#saving the data as csv locally to avoid running the Tweet API connection each time\n",
    "df5= pd.DataFrame(tweets, columns=['user','text'])\n",
    "df5.to_csv(path_or_buf = '/Users/sozeta/Desktop/DATASETS CSV/DF5.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Extracting tweets that include keyword 'Trump' from Jan 6 2021\n",
    "sleep_on_rate_limit=False\n",
    "tweets=[]\n",
    "search = \"#Trump -filter:retweets\"\n",
    "for tweet in tweepy.Cursor(api.search, q=search, limit=None, tweet_mode='extended', lang=\"en\", since=\"2021-01-06\", until=\"2021-01-07\").items(50000):\n",
    "    \n",
    "    try:\n",
    "        data=[tweet.user._json['screen_name'], tweet.full_text]\n",
    "        date= tuple(data)\n",
    "        tweets.append(data)\n",
    "        \n",
    "    except tweety.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df6= pd.DataFrame(tweets, columns=['user','text'])\n",
    "df6.to_csv(path_or_buf = '/Users/sozeta/Desktop/DATASETS CSV/DF6.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Extracting tweets that include keyword 'Trump' from Jan 7 2021\n",
    "search = \"#Trump -filter:retweets\"\n",
    "tweets=[]\n",
    "for tweet in tweepy.Cursor(api.search, q=search, limit=None, tweet_mode='extended', lang=\"en\", since=\"2021-01-07\", until=\"2021-01-08\").items(50000):\n",
    "    \n",
    "    try:\n",
    "        data=[tweet.user._json['screen_name'], tweet.full_text]\n",
    "        date= tuple(data)\n",
    "        tweets.append(data)\n",
    "        \n",
    "    except tweety.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df7= pd.DataFrame(tweets, columns=['user','text'])\n",
    "df7.to_csv(path_or_buf = '/Users/sozeta/Desktop/DATASETS CSV/DF7.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Extracting tweets that include keyword 'Trump' from Jan 8 2021\n",
    "search = \"#Trump -filter:retweets\"\n",
    "tweets=[]\n",
    "for tweet in tweepy.Cursor(api.search, q=search, limit=None, tweet_mode='extended', lang=\"en\", since=\"2021-01-08\", until=\"2021-01-09\").items(50000):\n",
    "    \n",
    "    try:\n",
    "        data=[tweet.user._json['screen_name'], tweet.full_text]\n",
    "        date= tuple(data)\n",
    "        tweets.append(data)\n",
    "        \n",
    "    except tweety.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df8= pd.DataFrame(tweets, columns=['user','text'])\n",
    "df8.to_csv(path_or_buf = '/Users/sozeta/Desktop/DATASETS CSV/DF8.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have all my data locally stored I will go ahead and create custom methods for cleaning the tweets stored in each dataframe in preparation for the word analysis I want to engage in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the tweets\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#function for removing a pattern in a string\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)        \n",
    "    return input_txt\n",
    "\n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    \n",
    "    #remove special characters, numbers\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from emot.emo_unicode import EMOTICONS\n",
    "\n",
    "#create set of stopwords and add my own\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_words = ['trump', 'donald','#trump','#donald','#donaldtrump','&amp;','amp','donaldtrump','likeamp']\n",
    "stop_words.update(extra_words)\n",
    "len(stop_words)\n",
    "\n",
    "def tweet_prep(tweets):\n",
    "    #remove apostrophes\n",
    "    tweets.text = np.vectorize(remove_pattern)(tweets.text, \"'\")\n",
    "    tweets.text = np.vectorize(remove_pattern)(tweets.text, \"|\")\n",
    "    #remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    tweets.text = np.vectorize(remove_pattern)(tweets.text, emoji_pattern)\n",
    "    \n",
    "    #remove punctuation\n",
    "    tweets.text = tweets.text.str.replace('[^\\w\\s]','')\n",
    "    tweets.text = tweets.text.str.replace('\\n','')\n",
    "    tweets.text = tweets.text.str.replace('  ','')\n",
    "    #lowerlcase letters\n",
    "    tweets.text = tweets.text.apply(lambda x: x.lower())\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "import en_core_web_sm\n",
    "sp= spacy.load(\"en_core_web_sm\")\n",
    "lookups = Lookups()\n",
    "lemm = Lemmatizer(lookups)\n",
    "\n",
    "#Function that converts each word in a list to its lemma version\n",
    "def lemma_function(tweets):\n",
    "    dummy = []\n",
    "    incoming=' '.join(tweets.text)\n",
    "    for word in sp(incoming):\n",
    "        dummy.append(word.lemma_)\n",
    "    #remove items from list that are not useful\n",
    "    for item in dummy[:]:\n",
    "        if (item==\"-PRON-\" or item=='  ' or item=='   '  or len(item)<2 or item in stop_words):\n",
    "            dummy.remove(item)\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions defined above will clear the Twitter data sufficiently in order for me to analyze word distribution across the tweets.\n",
    "\n",
    "Now I will import my data from the CSV files and then apply the cleaning functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Creating a pandas dataframe for each data file\n",
    "df5=pd.read_csv(\"desktop/DATASETS CSV/DF5.csv\")\n",
    "df6=pd.read_csv(\"desktop/DATASETS CSV/DF6.csv\")\n",
    "df7=pd.read_csv(\"desktop/DATASETS CSV/DF7.csv\")\n",
    "df8=pd.read_csv(\"desktop/DATASETS CSV/DF8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Cleaning the tweets saved in each dataframe\n",
    "df5['text']=clean_tweets(df5['text'])\n",
    "df6['text']=clean_tweets(df6['text'])\n",
    "df7['text']=clean_tweets(df7['text'])\n",
    "df8['text']=clean_tweets(df8['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Creating a list of the lemma version of each word in each tweet after proper cleaning of the tweet\n",
    "#the \"clean_tweets\" function updates the original tweet to remove extraneous information\n",
    "#the \"tweet_prep\" function returns a new object that excludes information that is not pertinent to this project but could be pertinent elsewhere\n",
    "words5=lemma_function(tweet_prep(df5))\n",
    "words6=lemma_function(tweet_prep(df6))\n",
    "words7=lemma_function(tweet_prep(df7))\n",
    "words8=lemma_function(tweet_prep(df8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using Counter objects to keep track of word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Creating a dictionary of words and their frequencies for each dataset(aka day)\n",
    "from collections import Counter \n",
    "Counter5 = Counter(words5)\n",
    "Counter6 = Counter(words6)\n",
    "Counter7 = Counter(words7)  \n",
    "Counter8 = Counter(words8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will visualize the word frequency distribution of the tweets for each day through Python's WordCloud art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most common words in the Jan 5 tweets around Donald Trump\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", prefer_horizontal=1, colormap='Set2', max_words=150, collocations=False).generate_from_frequencies(Counter5)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 5 2021:  Most common words in tweets about Donald Trump')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most common words in the Jan 6 tweets around Donald Trump\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", prefer_horizontal=1, max_words=150, colormap='Set2', collocations=False).generate_from_frequencies(Counter6)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 6 2021:  Most common words in tweets about Donald Trump')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most common words in the Jan 7 tweets around Donald Trump\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", prefer_horizontal=1, colormap='Set2', max_words=150, collocations=False).generate_from_frequencies(Counter7)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 7 2021:  Most common words in tweets about Donald Trump')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most common words in the Jan 8 tweets around Donald Trump\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", prefer_horizontal=1, colormap='Set2', max_words=150, collocations=False).generate_from_frequencies(Counter8)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 8 2021: Most common words in tweets about Donald Trump')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "After creating graphs for the most common words used in tweets about Donald Trump each day from Jan.5 to Jan.8, I want to create similar graphs that weigh the frequency of each word on a given day against its frequency on other days.  \n",
    "\n",
    "Below I create a basic algorithm to achieve that given the 4-day dataset we have:  \n",
    "    value_of_word_day1= frequency_day1 - ((frequency_day2+frequency_day3+frequency_day4)/3)</br>  \n",
    "    \n",
    "The resulting graphs highlight the newsworthy words of each day relating to Donald Trump as gathered through Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "DF_5 = {}\n",
    "for i in range(len(words5)):\n",
    "    count= Counter5[words5[i]]\n",
    "    commonality=0\n",
    "    if words5[i] in words6:\n",
    "        commonality+= Counter6[words5[i]]\n",
    "    if words5[i] in words7:\n",
    "        commonality+= Counter7[words5[i]]\n",
    "    if words5[i] in words8:\n",
    "        commonality+= Counter8[words5[i]]\n",
    "    if commonality==0:\n",
    "        value=count\n",
    "    else: \n",
    "        denominator=(commonality/3)\n",
    "        value=count-denominator\n",
    "    DF_5[words5[i]]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "DF_6 = {}\n",
    "for i in range(len(words6)):\n",
    "    count= Counter6[words6[i]]\n",
    "    commonality=0\n",
    "    if words6[i] in words6:\n",
    "        commonality+= Counter5[words6[i]]\n",
    "    if words6[i] in words7:\n",
    "        commonality+= Counter7[words6[i]]\n",
    "    if words6[i] in words8:\n",
    "        commonality+= Counter8[words6[i]]\n",
    "    if commonality==0:\n",
    "        value=count\n",
    "    else: \n",
    "        denominator=(commonality/3)\n",
    "        value=count-denominator\n",
    "    DF_6[words6[i]]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "DF_7 = {}\n",
    "for i in range(len(words7)):\n",
    "    count= Counter7[words7[i]]\n",
    "    commonality=0\n",
    "    if words7[i] in words5:\n",
    "        commonality+= Counter5[words7[i]]\n",
    "    if words7[i] in words6:\n",
    "        commonality+= Counter6[words7[i]]\n",
    "    if words7[i] in words8:\n",
    "        commonality+= Counter8[words7[i]]\n",
    "    if commonality==0:\n",
    "        value=count\n",
    "    else: \n",
    "        denominator=(commonality/3)\n",
    "        value=count-denominator\n",
    "    DF_7[words7[i]]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "DF_8 = {}\n",
    "for i in range(len(words8)):\n",
    "    count= Counter8[words8[i]]\n",
    "    commonality=0\n",
    "    if words8[i] in words5:\n",
    "        commonality+= Counter5[words8[i]]\n",
    "    if words8[i] in words6:\n",
    "        commonality+= Counter6[words8[i]]\n",
    "    if words8[i] in words7:\n",
    "        commonality+= Counter7[words8[i]]\n",
    "    if commonality==0:\n",
    "        value=count\n",
    "    else: \n",
    "        denominator=(commonality/3)\n",
    "        value=count-denominator\n",
    "    DF_8[words8[i]]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most newsworthy words in the Jan 5 tweets around Donald Trump\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", prefer_horizontal=1, colormap='Set1', max_font_size=60, max_words=150, collocations=False).generate_from_frequencies(DF_5)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 5 2021: Wordcloud of the daily news on Donald Trump')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most newsworthy words in the Jan 6 tweets around Donald Trump\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", prefer_horizontal=1, colormap='Set1', max_font_size=60, max_words=150, collocations=False).generate_from_frequencies(DF_6)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 6 2021: Wordcloud of the daily news on Donald Trump')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most newsworthy words in the Jan 7 tweets around Donald Trump\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", max_font_size=60, colormap='Set1', prefer_horizontal=1, max_words=150, collocations=False).generate_from_frequencies(DF_7)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 7 2021: Wordcloud of the daily news on Donald Trump')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#WordCloud art of the most newsworthy words in the Jan 8 tweets around Donald Trump\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", prefer_horizontal=1, colormap='Set1', max_font_size=60, max_words=150, collocations=False).generate_from_frequencies(DF_8)\n",
    "fig=plt.figure(figsize=(9, 9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('January 8 2021: Wordcloud of the daily news on Donald Trump')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
